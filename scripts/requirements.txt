
sudo apt-get update
sudo apt-get install -y apt-transport-https ca-certificates curl gnupg-agent software-properties-common 
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"
sudo apt-get update
sudo apt-get install -y docker-ce=5:19.03.12~3-0~ubuntu-bionic
sudo apt-mark hold docker-ce

sudo docker version

sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab


curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
cat <<EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list
deb https://apt.kubernetes.io/ kubernetes-xenial main
EOF
sudo apt-get update
sudo apt-get install -y kubelet=1.19.1-00 kubeadm=1.19.1-00 kubectl=1.19.1-00
sudo apt-mark hold kubelet kubeadm kubectl


vi config.yml
apiVersion: kubeadm.k8s.io/v1beta2
kind: ClusterConfiguration


sudo kubeadm init --config config.yml
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

kubectl apply -f https://docs.projectcalico.org/v3.14/manifests/calico.yaml

kubeadm token create --print-join-command
sudo kubeadm join ...

kubectl get namespaces
kubectl get pods -n kube-system
kubectl create namespace my-namespace

vi pod.yml
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
  - name: nginx
    image: nginx:1.14.2
    ports:
    - containerPort: 80
  restartPolicy: OnFailure

vi deployment.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-deployment
  labels:
    app: my-deployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: my-deployment
  template:
    metadata:
      labels:
        app: my-deployment
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80

kubectl get pods -o wide
kubectl drain <node name> --ignore-daemonsets --force
kubectl get pods -o wide
kubectl uncordon <node name>

sudo apt-get update && \
sudo apt-get install -y --allow-change-held-packages kubeadm=1.19.2-00

kubeadm version

kubectl drain <control plane node name> --ignore-daemonsets
sudo kubeadm upgrade plan
sudo kubeadm upgrade apply v1.19.2
kubectl uncordon <control plane node name>

sudo apt-get update && \
sudo apt-get install -y --allow-change-held-packages kubelet=1.19.2-00 kubectl=1.19.2-00
sudo systemctl daemon-reload
sudo systemctl restart kubelet

sudo apt-get update && \
sudo apt-get install -y --allow-change-held-packages kubeadm=1.19.2-00

kubeadm version
kubectl drain <worker 1 node name> --ignore-daemonsets
sudo kubeadm upgrade node
sudo apt-get update && \
sudo apt-get install -y --allow-change-held-packages kubelet=1.19.2-00 kubectl=1.19.2-00

sudo apt-get update && \
sudo apt-get install -y --allow-change-held-packages kubeadm=1.18.6-00

kubeadm version

From the control plane node, drain worker node 2.

kubectl drain <worker 2 node name> --ignore-daemonsets

Perform the upgrade on worker node 2.

sudo kubeadm upgrade node
sudo apt-get update && \
sudo apt-get install -y --allow-change-held-packages kubelet=1.18.6-00 kubectl=1.18.6-00
sudo systemctl daemon-reload
sudo systemctl restart kubelet

-------

ETCDCTL_API=3 etcdctl get cluster.name \
--endpoints=https://10.0.1.101:2379 \
--cacert=/home/cloud_user/etcd-certs/etcd-ca.pem \
--cert=/home/cloud_user/etcd-certs/etcd-server.crt \
--key=/home/cloud_user/etcd-certs/etcd-server.key

ETCDCTL_API=3 etcdctl snapshot save /home/cloud_user/etcd_backup.db \
--endpoints=https://10.0.1.101:2379 \
--cacert=/home/cloud_user/etcd-certs/etcd-ca.pem \
--cert=/home/cloud_user/etcd-certs/etcd-server.crt \
--key=/home/cloud_user/etcd-certs/etcd-server.key

sudo systemctl stop etcd
sudo rm -rf /var/lib/etcd

sudo ETCDCTL_API=3 etcdctl snapshot restore /home/cloud_user/etcd_backup.db \
--initial-cluster etcd-restore=https://10.0.1.101:2380 \
--initial-advertise-peer-urls https://10.0.1.101:2380 \
--name etcd-restore \
--data-dir /var/lib/etcd

sudo chown -R etcd:etcd /var/lib/etcd
sudo systemctl start etcd

ETCDCTL_API=3 etcdctl get cluster.name \
--endpoints=https://10.0.1.101:2379 \
--cacert=/home/cloud_user/etcd-certs/etcd-ca.pem \
--cert=/home/cloud_user/etcd-certs/etcd-server.crt \
--key=/home/cloud_user/etcd-certs/etcd-server.key

--------

vi pod.yml

apiVersion: v1
kind: Pod
metadata:
name: my-pod
spec:
containers:
- name: busybox
image: radial/busyboxplus:curl
command: ['sh', '-c', 'while true; do sleep 3600; done']


kubectl get pods -o wide --sort-by .spec.nodeName
kubectl get pods -n kube-system --selector k8s-app=calico-node


kubectl exec my-pod -c busybox -- echo "Hello, world!"
----

kubectl create deployment my-deployment --image=nginx --dry-run -o yaml > deployment.yml


-----
vi role.yml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: default
  name: pod-reader
rules:
- apiGroups: [""] # "" indicates the core API group
  resources: ["pods"]
  verbs: ["get", "watch", "list"]


vi rolebinding.yml

apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: pod-reader
  namespace: default
subjects:
- kind: User
  name: dev
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io
-----
vi my-serviceaccount.yml
apiVersion: v1
kind: ServiceAccount
metadata:
name: my-serviceaccount

kubectl create -f my-serviceaccount.yml
kubectl create sa my-serviceaccount2 -n default
kubectl get sa


vi sa-pod-reader.yml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: sa-pod-reader
  namespace: default
subjects:
  - kind: ServiceAccount
  name: my-serviceaccount
  namespace: default
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io

kubectl describe sa my-serviceaccount

---

kubectl get --raw /apis/metrics.k8s.io/
vi pod.yml
apiVersion: v1
kind: Pod 
metadata:
  name: my-pod
  labels:
    app: metrics-test
spec:
  containers:
  - name: busybox
    image: radial/busyboxplus:curl
    command: ['sh', '-c', 'while true; do sleep 3600; done']


kubectl top pod --sort-by cpu
kubectl top pod --selector app=metrics-test
kubectl apply -f https://raw.githubusercontent.com/linuxacademy/content-cka-resources/master/metrics-server-c

----
vi my-configmap.yml

apiVersion: v1
kind: ConfigMap
metadata:
  name: my-configmap
data:
  key1: Hello, world!
  key2: |
    Test
    multiple lines
    more lines

kubectl describe configmap my-configmap
echo -n 'secret' | base64
echo -n 'anothersecret' | base64

vi my-secret.yml
apiVersion: v1
kind: Secret
metadata:
  name: my-secret
type: Opaque
data:
  secretkey1: <base64 String 1>
  secretkey2: <base64 String 2>

vi volume-pod.yml
apiVersion: v1
kind: Pod
metadata:
name: volume-pod
spec:
  containers:
  - name: busybox
  image: busybox
  command: ['sh', '-c', 'while true; do sleep 3600; done']
  volumeMounts:
    - name: configmap-volume
    mountPath: /etc/config/configmap
    - name: secret-volume
    mountPath: /etc/config/secret
  volumes:
  - name: configmap-volume
    configMap:
      name: my-configmap
  - name: secret-volume
    secret:
      secretName: my-secret

kubectl exec volume-pod -- cat /etc/config/configmap/key1
kubectl exec volume-pod -- cat /etc/config/secret/secretkey2

----


vi big-request-pod.yml

apiVersion: v1
kind: Pod
metadata:
name: big-request-pod
spec:
containers:
- name: busybox
image: busybox
command: ['sh', '-c', 'while true; do sleep 3600; done']
resources:
requests:
cpu: "10000m"
memory: "128Mi"

kubectl create -f big-request-pod.yml

Check the pod status. It should never leave the Pending state since no worker nodes have enough resources to meet the
request.

kubectl get pod big-request-pod

Create a pod with resource requests and limits.

vi resource-pod.yml

apiVersion: v1
kind: Pod
metadata:
name: resource-pod
spec:
containers:
- name: busybox
image: busybox
command: ['sh', '-c', 'while true; do sleep 3600; done']
resources:
requests:
cpu: "250m"
memory: "128Mi"
limits:
cpu: "500m"
memory: "256Mi"


---

vi liveness-pod.yml

apiVersion: v1
kind: Pod
metadata:
name: liveness-pod
spec:
containers:
- name: busybox
image: busybox
command: ['sh', '-c', 'while true; do sleep 3600; done']
livenessProbe:
exec:
command: ["echo", "Hello, world!"]
initialDelaySeconds: 5
periodSeconds: 5


vi liveness-pod-http.yml

apiVersion: v1
kind: Pod
metadata:
  name: liveness-pod-http
spec:
  containers:
  - name: nginx
  image: nginx:1.19.1
  livenessProbe:
    httpGet:
    path: /
    port: 80
initialDelaySeconds: 5
periodSeconds: 5


vi startup-pod.yml
apiVersion: v1
kind: Pod
metadata:
name: startup-pod
spec:
containers:
- name: nginx
image: nginx:1.19.1
startupProbe:
httpGet:
path: /
port: 80
failureThreshold: 30
periodSeconds: 10

vi readiness-pod.yml

apiVersion: v1
kind: Pod
metadata:
name: readiness-pod
spec:
containers:
- name: nginx
image: nginx:1.19.1
readinessProbe:
httpGet:
path: /
port: 80
initialDelaySeconds: 5
periodSeconds: 5

---


vi always-pod.yml

apiVersion: v1
kind: Pod
metadata:
name: always-pod
spec:
restartPolicy: Always
containers:
- name: busybox
image: busybox
command: ['sh', '-c', 'sleep 10']


vi onfailure-pod.yml
apiVersion: v1
kind: Pod
metadata:
name: onfailure-pod
spec:
restartPolicy: OnFailure
containers:
- name: busybox
image: busybox
command: ['sh', '-c', 'sleep 10']

vi onfailure-pod.yml
apiVersion: v1
kind: Pod
metadata:
name: onfailure-pod
spec:
restartPolicy: OnFailure
containers:
- name: busybox
image: busybox
command: ['sh', '-c', 'sleep 10; this is a bad command that will fail']

vi never-pod.yml
apiVersion: v1
kind: Pod
metadata:
name: never-pod
spec:
restartPolicy: Never
containers:
- name: busybox
image: busybox
command: ['sh', '-c', 'sleep 10; this is a bad command that will fail']

---
vi multi-container-pod.yml

apiVersion: v1
kind: Pod
metadata:
name: multi-container-pod
spec:
containers:
- name: nginx
image: nginx
- name: redis
image: redis
- name: couchbase
image: couchbase

kubectl get pod multi-container-pod

Create a multi-container Pod that uses shared storage.

vi sidecar-pod.yml

apiVersion: v1
kind: Pod
metadata:
name: sidecar-pod
spec:
containers:
- name: busybox1
image: busybox
command: ['sh', '-c', 'while true; do echo logs data > /output/output.log; sleep 5; done']
volumeMounts:
- name: sharedvol
mountPath: /output
- name: sidecar
image: busybox
command: ['sh', '-c', 'tail -f /input/output.log']
volumeMounts:
- name: sharedvol
mountPath: /input
volumes:
- name: sharedvol
emptyDir: {}

---
vi init-pod.yml

apiVersion: v1
kind: Pod
metadata:
name: init-pod
spec:
containers:
- name: nginx
image: nginx:1.19.1
initContainers:
- name: delay
image: busybox
command: ['sleep', '30']

---
vi nodeselector-pod.yml
apiVersion: v1
kind: Pod
metadata:
name: nodeselector-pod
spec:
nodeSelector:
special: "true"
containers:
- name: nginx
image: nginx:1.19.1

vi nodename-pod.yml
apiVersion: v1
kind: Pod
metadata:
name: nodename-pod
spec:
nodeName: <node name>
containers:
- name: nginx
image: nginx:1.19.1

----

vi my-daemonset.yml
apiVersion: apps/v1
kind: DaemonSet
metadata:
name: my-daemonset
spec:
selector:
matchLabels:
app: my-daemonset
template:
metadata:
labels:
app: my-daemonset
spec:
containers:
- name: nginx
image: nginx:1.19.1

---
sudo vi /etc/kubernetes/manifests/my-static-pod.yml
apiVersion: v1
kind: Pod
metadata:
name: my-static-pod
spec:
containers:
- name: nginx
image: nginx:1.19.1

---
vi my-deployment.yml
apiVersion: apps/v1
kind: Deployment
metadata:
name: my-deployment
spec:
replicas: 3
selector:
matchLabels:
app: my-deployment
template:
metadata:
labels:
app: my-deployment
spec:
containers:
- name: nginx
image: nginx:1.19.1
ports:
- containerPort: 80

---

Edit a deployment spec, changing the number of replicas to scale the deployment up.

vi my-deployment.yml

...
spec:
replicas: 5


kubectl edit deployment my-deployment

...
spec:
containers:
- name: nginx
image: nginx:1.19.2
...

kubectl rollout status deployment.v1.apps/my-deployment
kubectl set image deployment/my-deployment nginx=nginx:broken --record
kubectl rollout history deployment.v1.apps/my-deployment
kubectl rollout undo deployment.v1.apps/my-deployment
kubectl rollout undo deployment.v1.apps/my-deployment --to-revision=<last working revision>


----
vi dnstest-pods.yml
apiVersion: v1
kind: Pod
metadata:
name: busybox-dnstest
spec:
containers:
- name: busybox
image: radial/busyboxplus:curl
command: ['sh', '-c', 'while true; do sleep 3600; done']
---
apiVersion: v1
kind: Pod
metadata:
name: nginx-dnstest
spec:
containers:
- name: nginx
image: nginx:1.19.2
ports:
- containerPort: 80

----
vi my-networkpolicy.yml

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
name: my-networkpolicy
namespace: np-test
spec:
podSelector:
matchLabels:
app: nginx
policyTypes:
- Ingress
- Egress

kubectl edit networkpolicy -n np-test my-networkpolicy

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
name: my-networkpolicy
namespace: np-test
spec:
podSelector:
matchLabels:
app: nginx
policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
  - namespaceSelector:
	matchLabels:
	team: np-test
	ports:
	  - port: 80
	protocol: TCP

----

vi deployment-svc-example.yml
apiVersion: apps/v1
kind: Deployment
metadata:
name: deployment-svc-example
spec:
replicas: 3
selector:
matchLabels:
app: svc-example
template:
metadata:
labels:
app: svc-example
spec:
containers:
- name: nginx
image: nginx:1.19.1
ports:
- containerPort: 80

kubectl create -f deployment-svc-example.yml

Create a ClusterIP Service to expose the deployment's Pods within the cluster network.

vi svc-clusterip.yml

apiVersion: v1
kind: Service
metadata:
name: svc-clusterip
spec:
type: ClusterIP
selector:
app: svc-example
ports:
- protocol: TCP
port: 80
targetPort: 80
kubectl create -f svc-clusterip.yml

vi pod-svc-test.yml
apiVersion: v1
kind: Pod
metadata:
name: pod-svc-test
spec:
containers:
- name: busybox
image: radial/busyboxplus:curl
command: ['sh', '-c', 'while true; do sleep 10; done']

vi svc-nodeport.yml
apiVersion: v1
kind: Service
metadata:
name: svc-nodeport
spec:
type: NodePort
selector:
app: svc-example
ports:
- protocol: TCP
port: 80
targetPort: 80
nodePort: 30080

----
kubectl exec pod-svc-test -- curl svc-clusterip
kubectl exec -n new-namespace pod-svc-test-new-namespace -- curl svc-clusterip.default.svc.cluster.local

----

vi my-ingress.yml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
name: my-ingress
spec:
rules:
- http:
paths:
- path: /somepath
pathType: Prefix
backend:
service:
name: svc-clusterip
port:
number: 80

vi svc-clusterip.yml
apiVersion: v1
kind: Service
metadata:
name: svc-clusterip
spec:
type: ClusterIP
selector:
app: svc-example
ports:
- name: http
protocol: TCP
port: 80
targetPort: 80

kubectl apply -f svc-clusterip.yml

Edit the Ingress to use the port name.

vi my-ingress.yml

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
name: my-ingress
spec:
rules:
- http:
paths:
- path: /somepath
pathType: Prefix
backend:
service:
name: svc-clusterip
port:
name: http

kubectl apply -f my-ingress.yml

Check the status of the ingress again.

kubectl describe ingress my-ingress

----


vi volume-pod.yml
apiVersion: v1
kind: Pod
metadata:
name: volume-pod
spec:
restartPolicy: Never
containers:
- name: busybox
image: busybox
command: ['sh', '-c', 'echo Success! > /output/success.txt']
volumeMounts:
- name: my-volume
mountPath: /output
volumes:
- name: my-volume
hostPath:
path: /var/data

vi shared-volume-pod.yml
apiVersion: v1
kind: Pod
metadata:
name: shared-volume-pod
spec:
containers:
- name: busybox1
image: busybox

command: ['sh', '-c', 'while true; do echo Success! > /output/output.txt; sleep 5; done']
volumeMounts:
- name: my-volume
mountPath: /output
- name: busybox2
image: busybox
command: ['sh', '-c', 'while true; do cat /input/output.txt; sleep 5; done']
volumeMounts:
- name: my-volume
mountPath: /input
volumes:
- name: my-volume
emptyDir: {}

----


vi localdisk-sc.yml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: localdisk
provisioner: kubernetes.io/no-provisioner
allowVolumeExpansion: true

vi my-pv.yml
kind: PersistentVolume
apiVersion: v1
metadata:
  name: my-pv
spec:
  storageClassName: localdisk
  persistentVolumeReclaimPolicy: Recycle
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: /var/output


vi my-pvc.yml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc
spec:
  storageClassName: localdisk
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 100Mi

vi pv-pod.yml
apiVersion: v1
kind: Pod
metadata:
  name: pv-pod
spec:
  restartPolicy: Never
  containers:
  - name: busybox
    image: busybox
    command: ['sh', '-c', 'echo Success! > /output/success.txt']
    volumeMounts:
    - name: pv-storage
      mountPath: /output
  volumes:
  - name: pv-storage
    persistentVolumeClaim:
      claimName: my-pvc

kubectl create -f pv-pod.yml
kubectl edit pvc my-pvc --record

...
spec:
...
resources:
requests:
storage: 200Mi

----

sudo systemctl stop kubelet
sudo systemctl disable kubelet
sudo systemctl status kubelet

sudo journalctl -u kubelet

vi troubleshooting-pod.yml
apiVersion: v1
kind: Pod
metadata:
  name: troubleshooting-pod
spec:
  containers:
  - name: busybox
    image: busybox
    command: ['sh', '-c', 'while true; do sleep 5; done']

kubectl exec troubleshooting-pod -c busybox --stdin --tty -- /bin/sh

vi logs-pod.yml
apiVersion: v1
kind: Pod
metadata:
  name: logs-pod
spec:
  containers:
  - name: busybox
    image: busybox
    command: ['sh', '-c', 'while true; do echo Here is my output!; sleep 5; done']


kubectl logs logs-pod -c busybox

kubectl get pods -n kube-system
kubectl logs -n kube-system <kube-proxy_POD_NAME>
kubectl get pods -n kube-system

vi nginx-netshoot.yml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-netshoot
  labels:
    app: nginx-netshoot
spec:
  containers:
  - name: nginx
    image: nginx:1.19.1

---

apiVersion: v1
kind: Service
metadata:
  name: svc-netshoot
spec:
  type: ClusterIP
  selector:
    app: nginx-netshoot
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80


vi netshoot.yml
apiVersion: v1
kind: Pod
metadata:
  name: netshoot
spec:
  containers:
  - name: netshoot
    image: nicolaka/netshoot
    command: ['sh', '-c', 'while true; do sleep 5; done']

kubectl exec --stdin --tty netshoot -- /bin/sh

curl svc-netshoot
ping svc-netshoot
nslookup svc-netshoot

